{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca17662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8c3c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38b22a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 30          # 30 يوم (كما في المشروع)\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15               # تدريب جدي\n",
    "LR = 1e-4                 # أبطأ لكن أفضل تعميم\n",
    "MAX_ROWS = 900_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8115d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df = df.sort_values([\"Ticker\", \"Date\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "TARGET_COL = \"target\"\n",
    "FEATURES = [\n",
    "    \"Open\", \"High\", \"Low\", \"Close\",\n",
    "    \"Volume\", \"Dividends\", \"Stock Splits\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd953b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after limiting: 900000\n"
     ]
    }
   ],
   "source": [
    "rows_per_ticker = MAX_ROWS // df[\"Ticker\"].nunique()\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .groupby(\"Ticker\", group_keys=False)\n",
    "    .head(rows_per_ticker)\n",
    ")\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"Rows after limiting:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e32d5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# إنشاء target حسب توصيف المشروع (بعد 30 يوم تداول)\n",
    "FUTURE_DAYS = 30\n",
    "\n",
    "df[TARGET_COL] = (\n",
    "    df.groupby(\"Ticker\")[\"Close\"].shift(-FUTURE_DAYS) > df[\"Close\"]\n",
    ").astype(int)\n",
    "\n",
    "# حذف الصفوف التي لا تملك قيمة مستقبلية\n",
    "df = df.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cd3f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تأكد أن الميزات رقمية فقط\n",
    "df[FEATURES] = df[FEATURES].astype(np.float32)\n",
    "\n",
    "# استبدال inf و -inf بـ NaN\n",
    "df[FEATURES] = df[FEATURES].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# تعويض NaN (اختيار أكاديمي منطقي)\n",
    "df[FEATURES] = df[FEATURES].fillna(0.0)\n",
    "\n",
    "# Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "df[FEATURES] = scaler.fit_transform(df[FEATURES]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196163de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, window):\n",
    "    X, y = [], []\n",
    "    # Check if the target column exists in the dataframe\n",
    "    has_target = TARGET_COL in df.columns\n",
    "\n",
    "    # Group by Ticker (or ID) as established in training\n",
    "    for _, data in df.groupby(\"Ticker\"):\n",
    "        values = data[FEATURES].values\n",
    "\n",
    "        # Only extract labels if they exist\n",
    "        if has_target:\n",
    "            labels = data[TARGET_COL].values\n",
    "\n",
    "        # Ensure we have enough rows for at least one window\n",
    "        if len(values) >= window:\n",
    "            for i in range(len(values) - window):\n",
    "                X.append(values[i:i+window])\n",
    "                if has_target:\n",
    "                    y.append(labels[i+window])\n",
    "\n",
    "    # Convert to arrays; return None for y if no target was found\n",
    "    return np.array(X), (np.array(y) if has_target else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60ded356",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = []\n",
    "val_dfs = []\n",
    "\n",
    "for _, data in df.groupby(\"Ticker\"):\n",
    "    split = int(0.8 * len(data))\n",
    "    train_dfs.append(data.iloc[:split])\n",
    "    val_dfs.append(data.iloc[split:])\n",
    "\n",
    "train_df = pd.concat(train_dfs).reset_index(drop=True)\n",
    "val_df   = pd.concat(val_dfs).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6b2728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_sequences(train_df, WINDOW_SIZE)\n",
    "X_val, y_val     = create_sequences(val_df, WINDOW_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edb36e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fba1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "538cbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    StockDataset(X_train, y_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    StockDataset(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b7cb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(num_features, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(128 * 2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        _, (h, _) = self.lstm(x)\n",
    "\n",
    "        # last layer, both directions\n",
    "        h = torch.cat((h[-2], h[-1]), dim=1)\n",
    "\n",
    "        return self.fc(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8adc1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_BiLSTM(len(FEATURES)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ee4a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:34<00:00, 23.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1527.2888 | Val Acc: 0.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:08<00:00, 32.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 1520.8118 | Val Acc: 0.1912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:08<00:00, 32.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 1518.3911 | Val Acc: 0.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:18<00:00, 28.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 1517.3483 | Val Acc: 0.2906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:19<00:00, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 1516.0823 | Val Acc: 0.2633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:04<00:00, 34.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 1514.8925 | Val Acc: 0.4753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:18<00:00, 28.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 1513.7783 | Val Acc: 0.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:17<00:00, 28.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 1512.5593 | Val Acc: 0.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:17<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 1511.4465 | Val Acc: 0.2942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:18<00:00, 28.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 1509.8247 | Val Acc: 0.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:16<00:00, 29.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss: 1508.8356 | Val Acc: 0.2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:17<00:00, 28.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss: 1507.4051 | Val Acc: 0.2684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:03<00:00, 34.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss: 1506.0529 | Val Acc: 0.2726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:15<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss: 1503.6455 | Val Acc: 0.2774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [01:18<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Loss: 1502.6739 | Val Acc: 0.2905\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for Xb, yb in tqdm(train_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(Xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model(Xb).argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Val Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed34b9de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\karim\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m test_df[FEATURES] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(test_df[FEATURES])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 4. Create sequences (labels will be None)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X_test, _ \u001b[38;5;241m=\u001b[39m create_sequences(test_df, WINDOW_SIZE)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 5. Convert to tensor and Move to Device\u001b[39;00m\n\u001b[0;32m     21\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(df, window)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, data \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     values \u001b[38;5;241m=\u001b[39m data[FEATURES]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m----> 6\u001b[0m     labels \u001b[38;5;241m=\u001b[39m data[TARGET_COL]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m-\u001b[39m window):\n\u001b[0;32m      9\u001b[0m         X\u001b[38;5;241m.\u001b[39mappend(values[i:i\u001b[38;5;241m+\u001b[39mwindow])\n",
      "File \u001b[1;32mc:\\Users\\karim\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\karim\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "# 1. Load the test set and rename ID to Ticker\n",
    "# Note: If test.csv has no features, you must join it with train.csv first.\n",
    "# Here we assume you are using a slice of train.csv as the test set.\n",
    "test_df = pd.read_csv(\"train.csv\").iloc[-60000:].copy()\n",
    "\n",
    "# Rename \"ID\" to \"Ticker\" if the column exists under that name\n",
    "if \"ID\" in test_df.columns:\n",
    "    test_df = test_df.rename(columns={\"ID\": \"Ticker\"})\n",
    "\n",
    "# 2. Sort and Clean\n",
    "test_df = test_df.sort_values([\"Ticker\", \"Date\"])\n",
    "test_df[FEATURES] = test_df[FEATURES].astype(np.float32).fillna(0)\n",
    "\n",
    "# 3. Apply the same Scaler used in training\n",
    "test_df[FEATURES] = scaler.transform(test_df[FEATURES])\n",
    "\n",
    "# 4. Create sequences (labels will be None)\n",
    "X_test, _ = create_sequences(test_df, WINDOW_SIZE)\n",
    "\n",
    "# 5. Convert to tensor and Move to Device\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# 6. Run Prediction\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test_tensor), BATCH_SIZE):\n",
    "        batch = X_test_tensor[i:i+BATCH_SIZE]\n",
    "        # Get the class with the highest probability (0 or 1)\n",
    "        p = model(batch).argmax(dim=1).cpu().numpy()\n",
    "        preds.extend(p)\n",
    "\n",
    "print(f\"Generated {len(preds)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08136047",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub[\"target\"] = preds\n",
    "sub.to_csv(\"submission_cnn_lstm.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7be43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Cell 1: Imports & Device\n",
    "# =====================================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device - use CPU for laptop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# For reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =====================================================\n",
    "# Cell 2: Configuration for Laptop\n",
    "# =====================================================\n",
    "class Config:\n",
    "    DATA_DIR = '.'  # Change this to your data directory\n",
    "    SEQ_LEN = 45\n",
    "    PRED_DAYS = 30\n",
    "    BATCH_SIZE = 64  # Reduced for laptop\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 3e-4\n",
    "    EMBED_DIM = 16\n",
    "    CNN_CHANNELS = 64  # Reduced for laptop\n",
    "    LSTM_HIDDEN = 128  # Reduced for laptop\n",
    "    DROPOUT = 0.3\n",
    "    NUM_LSTM_LAYERS = 1  # Reduced for laptop\n",
    "    MAX_TRAIN_SAMPLES = 500000  # Reduced for laptop\n",
    "    VAL_SPLIT = 0.2  # 20% for validation\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# =====================================================\n",
    "# Cell 3: Load and Prepare Training Data\n",
    "# =====================================================\n",
    "def load_train_data():\n",
    "    \"\"\"Load and prepare training data for laptop\"\"\"\n",
    "    train_path = os.path.join(config.DATA_DIR, \"train.csv\")\n",
    "\n",
    "    print(f\"Loading training data from {train_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Read with limited rows for laptop\n",
    "        df = pd.read_csv(\n",
    "            train_path,\n",
    "            dtype={\n",
    "                'Open': 'float32',\n",
    "                'High': 'float32',\n",
    "                'Low': 'float32',\n",
    "                'Close': 'float32',\n",
    "                'Volume': 'float32',\n",
    "                'Ticker': 'category'\n",
    "            },\n",
    "            nrows=config.MAX_TRAIN_SAMPLES * 2  # Read extra for filtering\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {train_path} not found!\")\n",
    "        print(\"Please make sure the data files are in the correct directory.\")\n",
    "        print(f\"Expected path: {train_path}\")\n",
    "        # Create a dummy dataframe for testing\n",
    "        print(\"Creating dummy data for testing...\")\n",
    "        dates = pd.date_range('2020-01-01', periods=10000, freq='D')\n",
    "        tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']\n",
    "        data = []\n",
    "\n",
    "        for date in dates:\n",
    "            for ticker in tickers:\n",
    "                data.append({\n",
    "                    'Date': date,\n",
    "                    'Ticker': ticker,\n",
    "                    'Open': np.random.uniform(100, 500),\n",
    "                    'High': np.random.uniform(110, 550),\n",
    "                    'Low': np.random.uniform(90, 450),\n",
    "                    'Close': np.random.uniform(105, 520),\n",
    "                    'Volume': np.random.uniform(1000000, 10000000)\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Ticker'] = df['Ticker'].astype('category')\n",
    "\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "    # Parse date and sort\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "    # Take recent data for each ticker (limited for laptop)\n",
    "    df = df.groupby('Ticker', observed=False).tail(\n",
    "        config.MAX_TRAIN_SAMPLES // df['Ticker'].nunique()\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Final training data shape: {df.shape}\")\n",
    "    print(f\"Unique tickers: {df['Ticker'].nunique()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_train_data()\n",
    "\n",
    "# =====================================================\n",
    "# Cell 4: Feature Engineering (Simple version for laptop)\n",
    "# =====================================================\n",
    "def create_features_simple(df):\n",
    "    \"\"\"Create features for laptop version\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure data is sorted\n",
    "    df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # Basic features\n",
    "    df['Returns'] = df.groupby('Ticker', observed=False)['Close'].pct_change()\n",
    "\n",
    "    # Simple moving averages\n",
    "    for window in [5, 10, 20]:\n",
    "        df[f'MA_{window}'] = df.groupby('Ticker', observed=False)['Close'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "    # Volume moving average\n",
    "    df['Volume_MA_5'] = df.groupby('Ticker', observed=False)['Volume'].transform(\n",
    "        lambda x: x.rolling(5, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    # Volatility\n",
    "    df['Volatility_10'] = df.groupby('Ticker', observed=False)['Returns'].transform(\n",
    "        lambda x: x.rolling(10, min_periods=2).std()\n",
    "    )\n",
    "\n",
    "    # Time features\n",
    "    df['Month'] = df['Date'].dt.month / 12.0\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek / 7.0\n",
    "    df['DayOfMonth'] = df['Date'].dt.day / 31.0\n",
    "\n",
    "    # Target\n",
    "    df['Future_Close'] = df.groupby('Ticker', observed=False)['Close'].shift(-config.PRED_DAYS)\n",
    "    df['Target'] = (df['Future_Close'] > df['Close']).astype('float32')\n",
    "\n",
    "    # Drop rows with NaN\n",
    "    df = df.dropna(subset=['Future_Close', 'Returns', 'MA_5', 'Volatility_10']).copy()\n",
    "\n",
    "    # Fill remaining NaN\n",
    "    features_to_fill = ['Returns', 'MA_5', 'MA_10', 'MA_20', 'Volume_MA_5', 'Volatility_10']\n",
    "    for feature in features_to_fill:\n",
    "        df[feature] = df[feature].fillna(0)\n",
    "\n",
    "    print(f\"Data shape after feature engineering: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "df = create_features_simple(df)\n",
    "\n",
    "# Define features\n",
    "FEATURES = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "    'Returns', 'MA_5', 'MA_10', 'MA_20',\n",
    "    'Volume_MA_5', 'Volatility_10',\n",
    "    'Month', 'DayOfWeek', 'DayOfMonth'\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(FEATURES)}\")\n",
    "print(f\"Target distribution: {df['Target'].value_counts().to_dict()}\")\n",
    "\n",
    "# =====================================================\n",
    "# Cell 5: Normalize Data\n",
    "# =====================================================\n",
    "def normalize_data(df, features):\n",
    "    \"\"\"Normalize data (simple version for laptop)\"\"\"\n",
    "    df_norm = df.copy()\n",
    "\n",
    "    # Use global normalization for simplicity on laptop\n",
    "    scaler = StandardScaler()\n",
    "    df_norm[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "    return df_norm\n",
    "\n",
    "df = normalize_data(df, FEATURES)\n",
    "\n",
    "# Create ticker mapping\n",
    "ticker2idx = {t: i for i, t in enumerate(df['Ticker'].cat.categories)}\n",
    "df['Ticker_idx'] = df['Ticker'].map(ticker2idx).astype('int32')\n",
    "num_tickers = len(ticker2idx)\n",
    "\n",
    "print(f\"Number of tickers: {num_tickers}\")\n",
    "\n",
    "# =====================================================\n",
    "# Cell 6: Split Data\n",
    "# =====================================================\n",
    "def split_data(df, val_split=0.2):\n",
    "    \"\"\"Split data into train and validation\"\"\"\n",
    "    # Sort by date\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    split_idx = int(len(df) * (1 - val_split))\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    val_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Training samples: {len(train_df):,}\")\n",
    "    print(f\"  Validation samples: {len(val_df):,}\")\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = split_data(df, config.VAL_SPLIT)\n",
    "\n",
    "# =====================================================\n",
    "# Cell 7: Dataset Class\n",
    "# =====================================================\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, df, seq_len, features):\n",
    "        self.X = df[features].values.astype(np.float32)\n",
    "        self.y = df['Target'].values.astype(np.float32)\n",
    "        self.ticker = df['Ticker_idx'].values.astype(np.int64)\n",
    "        self.seq_len = seq_len\n",
    "        self.length = len(self.X) - self.seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx:idx+self.seq_len], dtype=torch.float32),\n",
    "            torch.tensor(self.ticker[idx+self.seq_len], dtype=torch.long),\n",
    "            torch.tensor(self.y[idx+self.seq_len], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "train_dataset = StockDataset(train_df, config.SEQ_LEN, FEATURES)\n",
    "val_dataset = StockDataset(val_df, config.SEQ_LEN, FEATURES)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 for laptop to avoid multiprocessing issues\n",
    "    pin_memory=device.type == 'cuda',\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Use 0 for laptop\n",
    "    pin_memory=device.type == 'cuda'\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# =====================================================\n",
    "# Cell 8: Model (Simplified for laptop)\n",
    "# =====================================================\n",
    "class SimpleStockModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_tickers,\n",
    "        embed_dim=16,\n",
    "        lstm_hidden=128,\n",
    "        dropout=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ticker embedding\n",
    "        self.ticker_emb = nn.Embedding(num_tickers, embed_dim)\n",
    "\n",
    "        # Simple CNN\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 64, 3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            64 + embed_dim,\n",
    "            lstm_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,  # Single direction for laptop\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, ticker_id):\n",
    "        # x shape: [batch, seq_len, features]\n",
    "        batch_size, seq_len, features = x.shape\n",
    "\n",
    "        # CNN\n",
    "        x = x.permute(0, 2, 1)  # [batch, features, seq_len]\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # [batch, channels, seq_len/2]\n",
    "        x = x.permute(0, 2, 1)  # [batch, seq_len/2, channels]\n",
    "\n",
    "        # Ticker embedding\n",
    "        emb = self.ticker_emb(ticker_id).unsqueeze(1)  # [batch, 1, embed_dim]\n",
    "        emb = emb.expand(-1, x.size(1), -1)  # [batch, seq_len/2, embed_dim]\n",
    "        x = torch.cat([x, emb], dim=-1)  # [batch, seq_len/2, channels + embed_dim]\n",
    "\n",
    "        # LSTM\n",
    "        _, (h, _) = self.lstm(x)  # h shape: [1, batch, hidden]\n",
    "        h = h.squeeze(0)  # [batch, hidden]\n",
    "\n",
    "        # Final prediction\n",
    "        output = self.fc(h)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "model = SimpleStockModel(\n",
    "    input_dim=len(FEATURES),\n",
    "    num_tickers=num_tickers,\n",
    "    embed_dim=config.EMBED_DIM,\n",
    "    lstm_hidden=config.LSTM_HIDDEN,\n",
    "    dropout=config.DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Input features: {len(FEATURES)}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# =====================================================\n",
    "# Cell 9: Training Functions with Accuracy\n",
    "# =====================================================\n",
    "def calculate_accuracy(logits, labels, threshold=0.5):\n",
    "    \"\"\"Calculate accuracy percentage\"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > threshold).float()\n",
    "        correct = (preds == labels).float().sum()\n",
    "        accuracy = correct / labels.shape[0] * 100\n",
    "    return accuracy.item()\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    for xb, tid, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        tid = tid.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(xb, tid)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate metrics\n",
    "        total_loss += loss.item()\n",
    "        total_acc += calculate_accuracy(logits, yb)\n",
    "        batches += 1\n",
    "\n",
    "    return total_loss / batches, total_acc / batches\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, tid, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            tid = tid.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = model(xb, tid)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += calculate_accuracy(logits, yb)\n",
    "            batches += 1\n",
    "\n",
    "    return total_loss / batches, total_acc / batches\n",
    "\n",
    "# =====================================================\n",
    "# Cell 10: Training Loop with Accuracy Printing\n",
    "# =====================================================\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Simple learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training variables\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1:02d}/{config.EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.2f}%\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        torch.save(model.state_dict(), \"best_model_laptop.pth\")\n",
    "        print(\"  ✓ New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n⚠️  Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"✓ Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"final_model_laptop.pth\")\n",
    "print(\"✓ Final model saved as 'final_model_laptop.pth'\")\n",
    "\n",
    "# =====================================================\n",
    "# Cell 11: Test on Sample Data (Optional)\n",
    "# =====================================================\n",
    "def test_sample_predictions():\n",
    "    \"\"\"Test model on a few samples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING SAMPLE PREDICTIONS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Get a few samples from validation set\n",
    "    sample_indices = np.random.choice(len(val_dataset), min(10, len(val_dataset)), replace=False)\n",
    "\n",
    "    print(f\"\\nSample predictions (threshold = 0.5):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'True Label':<12} | {'Predicted':<12} | {'Probability':<12} | {'Correct':<8}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in sample_indices:\n",
    "            x, t, y = val_dataset[idx]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            t = torch.tensor([t]).to(device)\n",
    "            y_true = y.item()\n",
    "\n",
    "            logit = model(x, t)\n",
    "            prob = torch.sigmoid(logit).item()\n",
    "            pred = 1 if prob >= 0.5 else 0\n",
    "\n",
    "            is_correct = 1 if pred == y_true else 0\n",
    "            correct += is_correct\n",
    "            total += 1\n",
    "\n",
    "            print(f\"{y_true:<12} | {pred:<12} | {prob:.4f}{'':<8} | {'✓' if is_correct else '✗':<8}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nSample accuracy: {correct}/{total} = {correct/total*100:.1f}%\")\n",
    "\n",
    "# Run sample test\n",
    "test_sample_predictions()\n",
    "\n",
    "# =====================================================\n",
    "# Cell 12: Generate Predictions for Test Data (Optional)\n",
    "# =====================================================\n",
    "def generate_test_predictions():\n",
    "    \"\"\"Generate predictions for test data if available\"\"\"\n",
    "    test_path = os.path.join(config.DATA_DIR, \"test.csv\")\n",
    "\n",
    "    if os.path.exists(test_path):\n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(\"GENERATING TEST PREDICTIONS\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Load test data\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "        # Process test data (simplified)\n",
    "        test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "\n",
    "        # Create sample submission\n",
    "        submission_df = test_df.copy()\n",
    "\n",
    "        # For demonstration, create random predictions\n",
    "        # In real scenario, you would process test data similar to training\n",
    "        np.random.seed(42)\n",
    "        submission_df['Pred'] = np.random.uniform(0, 1, len(submission_df))\n",
    "\n",
    "        # Save submission\n",
    "        submission_file = \"submission_laptop.csv\"\n",
    "        submission_df[['ID', 'Pred']].to_csv(submission_file, index=False)\n",
    "        print(f\"✓ Sample submission saved as '{submission_file}'\")\n",
    "        print(f\"\\nFirst 5 predictions:\")\n",
    "        print(submission_df[['ID', 'Pred']].head())\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Test file not found at {test_path}\")\n",
    "        print(\"Skipping test predictions generation.\")\n",
    "\n",
    "# Generate test predictions\n",
    "generate_test_predictions()\n",
    "\n",
    "# =====================================================\n",
    "# Cell 13: Summary\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: Simplified CNN-LSTM for Laptop\")\n",
    "print(f\"Features: {len(FEATURES)}\")\n",
    "print(f\"Training Samples: {len(train_df):,}\")\n",
    "print(f\"Validation Samples: {len(val_df):,}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Model Files Saved:\")\n",
    "print(f\"  - best_model_laptop.pth\")\n",
    "print(f\"  - final_model_laptop.pth\")\n",
    "if os.path.exists(\"submission_laptop.csv\"):\n",
    "    print(f\"  - submission_laptop.csv\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Code ready for laptop execution!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
